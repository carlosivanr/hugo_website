---
title: Linear Regression Pt. 3 - Casewise Diagnostics
author: Carlos Rodriguez
date: '2021-10-11'
slug: regression-diagnostics
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-11T21:11:38-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: false
type: book
weight: 30
#bibliography: [../../../../packages.bib]
#nocite: |
#  @R-car
#  @DSUR
---

```{r, warning=FALSE, message=FALSE, include=FALSE}
library(tidyverse)
library(kableExtra)
#library(car)
#library(QuantPsyc)
#library(broom)
data <- read.table(file = "~/hugo_website/content/how-to-guides/correlation-and-regression/2021-10-09-linear-regression-in-r/Supermodel.dat", header = TRUE)
multiple <- lm(SALARY ~ AGE + BEAUTY + YEARS, data = data)
```

After building our simple and multiple regression model, we turn our attention to casewise diagnostics to learn about which outliers are present in the sample and which data points have undue influence on our model which could affect the models stability. We will focus on the standardized residuals, Cook's distance, and leverage/hat values, but there are several other measures to assess model diagnostics. 

### Outliers and standardized residuals
Residuals help us understand how well the model fits the sample data. Standardized residuals are derived by dividing the non-standardized residuals by an estimate of their standard deviation. Standardized residuals can be obtained by applying the `rstandard()` function on our multiple regression model. Standardized residuals primarily serve two roles. First, they facilitate interpretation across different models because the units are in standard deviations rather than the unit of the outcome variable. Second, they serve as an indicator of outliers that may bias the estimated regression coefficients. A couple of general rules are that no more than 5% of the absolute values of the standardized residuals are greater than 2 and no more than 1% of the absolute values of the standardized residuals are greater than 2.5. In our example dataset, about 5.2% of the standardized residuals values are beyond the +/-2 boundary which is evidence that our model may not represent our outcome data well.
```{r}
# Create a column of standardized residuals
data$standardized.residuals <- rstandard(multiple)

# Create a boolean vector of large residuals; greater than 2 or less than -2 
data$large.residual <- data$standardized.residuals > 2 | data$standardized.residuals < -2

# Sum of large standardized residuals
sum(data$large.residual)

# Percentage of standardized residuals greater than 2 or less than -2
sum(data$large.residual)/length(data$large.residual) * 100

# Print table of cases where large.residual equals TRUE
kable(
  filter(data, large.residual == TRUE)
)
```

### Influential cases: Cook's distance
There are a few ways to determine which cases within a regression model have unde influence in the model parameters. One way is to calculate Cook's distance which has a straightforward interpretation - any value greater than 1 may be cause for concern. Cook's distance values can be obtained with the `cooks.distance()` function by passing the model as its input. With this dataset, there are no values greater than 1. This suggest that the model is stable across the sample because none of the cases exert undue influence on the model parameters.
```{r}
# Create a column of Cook's distance values
data$cooks.distance <- cooks.distance(multiple)

# Create a boolean vector of large residuals; greater than 2 or less than -2 
data$large.cooks.d <- data$cooks.distance > 1

# Sum of large Cook's distance
sum(data$large.cooks.d)
```

### Influential cases: Leverage/hat values
Leverage/hat values are an additional measure of influential cases. Leverage values can obtained by passing the regression model object to the `hatvalues()` function. Cases with values that are 2 or 3 times as large as (k + 1/n), where k = the number of predictors and n = the sample size, may have undue influence. With these data, values higher than 0.035 and 0.052, depending on how conservative you want to be. There are 25 cases with hat values 2 times greater than the average leverage value, and 3 cases with hat values greater than 3 times the average leverage value.
```{r}
# Create a column of leverage/hat values
data$leverage <- hatvalues(multiple)

# Average Leverage, # of predictors + 1 divided by n
((3 + 1)/231) * 2
((3 + 1)/231) * 3

# Create a boolean vector of large hat values
data$large.leverage <- data$leverage > ((3 + 1)/231) * 2

# Sum of large leverage 2, conservative
sum(data$large.leverage)

# Create a boolean vector of large hat values
data$large.leverage <- data$leverage > ((3 + 1)/231) * 3

# Sum of large leverage 3, less conservative
sum(data$large.leverage)

# Print table
kable(
  filter(data, large.leverage == TRUE) %>%
    select(SALARY, AGE, YEARS, BEAUTY, leverage, large.leverage)
  
)
```

### References
<div id="refs" class="references">

<div id="ref-DSUR">

Field, Andy, Jeremy Miles, and Zoe Field. 2012. *Discovering Statistics Using R*. Sage.

</div>

</div>


<!-- ### Diagnostics -->
<!-- ```{r, include = FALSE} -->
<!-- data$residuals <- resid(multiple) -->
<!-- data$standardized.residuals <- rstandard(multiple) -->
<!-- data$studentized.residuals <- rstudent(multiple) -->
<!-- data$cooks.distance <- cooks.distance(multiple) -->
<!-- data$dfbeta <- dfbeta(multiple) -->
<!-- data$dffits <- dffits(multiple) -->
<!-- data$leverage <- hatvalues(multiple) -->
<!-- data$covariance.ratios <- covratio(multiple) -->
<!-- data$fitted <- multiple$fitted.values -->
<!-- ``` -->
<!-- ```{r} -->
<!-- ``` -->


<!-- ### Covariance ratios -->
<!-- Several values are below the bottom boundary, below .948 and above 1.052. There are many values here that are well below this lower bound. But the cooks distance for these values is OK.  -->
<!-- CVR > 1 + (3 * (k+1)/n) -->
<!-- CVR < 1 - (3 * (k+1)/n) -->

<!-- `* Calculate the lower and upper bound limits of the covariance ratio. Values outside of the bounds could be problematic.` -->
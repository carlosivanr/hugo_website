---
title: K Nearest Neighbors
author: Carlos Rodriguez
date: '2021-10-26'
slug: k-nearest-neighbors
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-26T20:59:00-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
type: book
weight: 75
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
library(kableExtra)
library(reticulate)

#conda_list() #lists conda environments
use_condaenv("r-reticulate")
```
The following describes a Python-based workflow for using k-nearest neighbors (KNN) to solve a classification problem. KNN is method of classifying values into a predefined number of groups based on their proximity to neighboring points. The number of neighboring points used in classification is referred to as K. In this example, we will use a fabricated data set in which the variables have been anonymized and our task to classify the data. All data were sourced from the Python for Data Science and Machine Learning Bootcamp by Pieran Data hosted on Udemy.com.

### Import packages
```{python, packages}
# Data Analysis & Visualization Packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# KNN Packages
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
```

### Load data
```{python, load-data}
df = pd.read_csv("KNN_Project_Data")
```

### Inspect data
Notice the column names are anonymized into 4 random letters. We have 10 columns of numeric variables and a Target Class variable consisting of zeros or ones.
```{python, head, eval = FALSE}
df.head(3)
```
```{r, display-head, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$df
kable(head(df, n=3))
```

### Exploratory Data Analysis
Before using the KNN algorithm, we can produce a pairplot of all of the variables. With this number of variable, the pairplot will fairly large and since the data are fabricated, there is not a lot of insight that can be gathered. However, this step is shown as a critical step in visually inspecting data.
```{python, eda, fig.cap = "Pairplot of all variables."}
sns.pairplot(df,hue='TARGET CLASS',palette='coolwarm')
```

### Standardize Variables
Our first step in preparing the data for KNN is to standardize the variables. In sci-kit learn, the `StandardScaler()` function can easily accomplish this task. We first create an instance of `StandardScaler()`. We don't want to standardize the Target Class variable so the `.drop()` method comes in handy here to standardize the features only. The axis=1 call is to specify that we want to drop the data down the column. In python the axis=0 represents rows and axis=1 represents columns. Next we will use the `.transform()` method to standardize and scale the features. Once again, the Target Class variable is excluded from this step. Finally, we convert the scaled features into a data frame using all of the columns except for the last Target Class variable with slice notation.
```{python, standardize variables}
scaler = StandardScaler()
scaler.fit(df.drop('TARGET CLASS', axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
```
```{python, eval = FALSE}
df_feat.head()
```
```{r, display-head_feat, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$df
kable(head(df, n=3))
```

### Split data into testing and training sets
Our next task is to create separate training and testing data sets. Our X training and testing sets will be created from the scaled features, and our y training and testing sets will be created from the Target Class variables. Finally, we will split the data in such a way that 30% of the data is retained for testing and the rest is left over for training.
```{python, train-test split}
X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'], test_size=0.30)
```

### Classify using KNN
Now we move on to creating a KNN model. We begin by first instantiating a KNN object. Since we do not know the optimal number of K we can begin with one. Later on, we will use the elbow method to determine a better K value. Once we instantiate the KNN model, we can use the training data to fit the model.
```{python}
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
```

### Predictions and Evaluations
After fitting our training data, we can then use the X test data to make predictions on data that the model has not "seen".
```{python}
predictions = knn.predict(X_test)
```


**Classification Report** - Next, we can compare our predicted and actual y values with the `classification_report()` function. Notice that our model is 78% accurate.
```{python, eval = FALSE}
print(classification_report(y_test,predictions))
```
```{python, prep-report, echo = FALSE}
# This chunk will save the classification report as a dictionary which can then be
# transposed and saved as a pandas data frame. Then the data frame can be loaded
# into the R workspace to display it with the kable() function, and make the out-
# put more readable.
report = classification_report(y_test, predictions, output_dict=True)
df = pd.DataFrame(report).transpose()
```
```{r, classification report, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$df
kable(df)
```

**Confusion Matrix** - The confusion matrix is one way of representing the performance of a classifier. In the first column, the matrix displays the number of correct classifications of the "0" label and the number of incorrect classifications of the "0" label. The second column displays the incorrect number of classifications of the "1" label and the number of correct classifications of the "0" label.
```{python, eval = FALSE}
confusion_matrix(y_test, predictions)
```
```{python, prep-c-mat, echo = FALSE}
cmat = confusion_matrix(y_test, predictions)
```
```{r, display-cmat, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$cmat
kable(df)
```


### Choosing a K value
One way to choose a K value is to use the elbow method. In this method, we will write a for-loop to run a KNN model for multiple values of K. In this case, we will run 40 iterations of KNN models with K values from 1 through 40, and then plot the error rate for each iteration. The optimal value of k is to select the lowest value of k associated with the lowest error rate. One way is to visualize a horizontal asymptote. The plot will show a point where the error rate does not reduce, so we want to select a value of k corresponding to this. If we look at the plot error rates stop fluctuating after about k = 30.
```{python, fig.cap = "Error rate as a function of k-value. This plot visualizes the elbow criterion to determine the optimal k-value."}
error_rate = []

for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    predictions_i = knn.predict(X_test)
    error_rate.append(np.mean(predictions_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
```

### Retrain with new K value
Now that we know that K=30 will reduce the error rate, we can re-run the knn algorithm and compare classification performance.
```{python}
# NOW WITH K=30
knn = KNeighborsClassifier(n_neighbors=30)

knn.fit(X_train,y_train)
predictions = knn.predict(X_test)
```

**Classification Report**
Our knn model with K=30 has improved accuracy. We went from 78% to 84% suggesting a modest improvement in accuracy.
```{python, eval = FALSE}
print(classification_report(y_test,predictions))
```
```{python, prep-report2, echo = FALSE}
# This chunk will save the classification report as a dictionary which can then be
# transposed and saved as a pandas data frame. Then the data frame can be loaded
# into the R workspace to display it with the kable() function, and make the out-
# put more readable.
report = classification_report(y_test, predictions, output_dict=True)
df = pd.DataFrame(report).transpose()
```
```{r, classification report2, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$df
kable(df)
```

**Confusion Matrix**
```{python, eval = FALSE}
confusion_matrix(y_test, predictions)
```
```{python, prep-c-mat2, echo = FALSE}
cmat = confusion_matrix(y_test, predictions)
```
```{r, display-cmat2, echo = FALSE}
#R code to display pandas table
df <- reticulate::py$cmat
kable(df)
```

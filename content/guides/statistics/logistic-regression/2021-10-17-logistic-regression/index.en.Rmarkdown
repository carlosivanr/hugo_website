---
title: Logistic Regression Pt. 1 - One Predictor Variable
author: Carlos Rodriguez
date: '2021-10-17'
slug: logistic-regression
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-17T18:49:28-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
type: book
weight: 75
draft: false
# bibliography: [../../../../packages.bib]
# nocite: |
#   @R-car
#   @DSUR
#   @R-tidyverse
---


Logistic regression is a statistical technique to understand the relationship between a dichotomous categorical outcome and categorical or continuous predictor variables. Examples of logistic regression include analyses to determine factors associated with whether or not someone survived a hospital stay or whether or not a medical intervention was successful or not. For this guide, we will work with example data in which eels are used as an experimental treatment to cure a medical condition. Our outcome will fall into one of two possibilities, Cured or Not Cured, and our predictor is whether someone received the Intervention or not.

```{r, warning=FALSE, message=FALSE}
library(car)
library(kableExtra)
library(tidyverse)
```

### Load data
```{r}
#load data
eelData <- read.delim("eel.dat", header = TRUE)
```

### Inspect data
```{r}
#look at first 6 cases of data
kable(head(eelData))
```

### Prepare data
In R, logistic regression models can utilize a dichotomous categorical variable as the dependent variable or they can utilize a binary indicator variable consisting of 0s and 1s. For this example, we will use the categorical variable to facilitate the interpretation of the output. First, convert the Cured and Intervention variables to factored variables. Additionally, we will need to order the levels so that Not Cured and No Treatment are the baseline (reference) categories. The first level of a factored variable will become the reference category in the model.
```{r}
eelData$Cured <- factor(eelData$Cured, levels = c("Not Cured", "Cured"))
eelData$Intervention <- factor(eelData$Intervention, levels = c("No Treatment", "Intervention"))
```

### Visualize data
The data consist of a dichotomous outcome variable, Cured or Not Cured, and a dichotomous predictor variable, Intervention or No Treatment. One way to visualize these data is through a stacked bar chart where we can count each possible combination of Cured and Intervention values to get an idea of the counts.
```{r}
colors <- c( "#440154FF","#1565c0")
ggplot(eelData, aes(x = Cured, color = Intervention, fill = Intervention)) +
  stat_count(alpha = 0.7) +
  scale_color_manual(values = colors) +
  scale_fill_manual(values = colors) +
  theme_minimal() +
  theme(axis.line = element_line(color = "grey70"))
```


### Model the data with one predictor variable
For our first logistic regression model, we will attempt to predict Cured from Intervention only. For a logistic regression analysis, we will use the `glm()` function, specify `binomial()` as the family argument, and then specify the data. Lastly, we will use the `summary()` to display the output.
```{r}
eel_model.1 <- glm(Cured ~ Intervention, family = binomial(), data = eelData)

summary(eel_model.1)
```

### Walkthrough of R's logistic regression output
The output of the `summary()` function can be divided into four sections. 
  * The first section is the "**Call:**," which is simply a reproduction of the `glm()` function used to create the model. 
  * The next section, "**Deviance Residuals:**," contains information about the spread of the residuals which are the differences between the model predictions and the observed values. 
  * In the third, "**Coefficients:**," section we find information regarding the model parameters. The same section will contain a standard error, z-value, and p-value.
  * Finally, the last section displays information about the **overall fit** of the model.

### Deviance statistics
The "Null deviance" value provides information on a model that does not contain any predictors other than the constant, which is akin to the mean in a simple linear regression model. In contrast, the "Residual deviance" value provides information regarding fit in the model that does contain our predictors. If our model fits the data well, then we would expect lower residuals deviance values compared to the null deviance values. The difference between the residual and null deviance values follows a chi-square distribution and can be tested for statistical significance. This is accomplished with the `pchisq()` function which calculates the area under the curve of a chi-distribution with the corresponding degrees of freedom. The result of this command is a p-value of 0.002 (rounded) which would indicate that including our predictor "Intervention" significantly improves the fit of the model compared to model that does not include our predictors.

```{r, include=FALSE}
#Just to prove what the null deviance is
# eel_model.0 <- glm(Cured ~ 1, data = eelData, family = binomial())
# summary(eel_model.0)
```


```{r}
#difference between model deviance statistics
chi_model <- eel_model.1$null.deviance - eel_model.1$deviance

#difference between model degrees of freedom
chidf <- eel_model.1$df.null - eel_model.1$df.residual

#one minus probability of chi_model given df
chisq.prob <- 1 - pchisq(chi_model, chidf)

#produce an output table
Intervention.Model <- c("Chi-square statistic", "Degrees of freedom", "p-value")
Value <- c(chi_model, chidf, chisq.prob)
kable(data.frame(Intervention.Model, Value))
```
```{r, include = FALSE, eval=FALSE}
# Computing R squared values, 
R2.hl <- chi_model/eel_model.1$null.deviance # Hosmer & Lemeshow
R.cs <- 1 - exp ((eel_model.1$deviance - eel_model.1$null.deviance)/113) # Cox & Snell
R.n <- R.cs /( 1- ( exp (-(eel_model.1$null.deviance/ 113)))) # Nagelkerke's R squared


# Function to produce pseudo R squareds
logistic_pseudoRs <- function(log_model){
  deviance <- log_model$deviance
  null_deviance <- log_model$null.deviance
  model_N <- length(log_model$fitted.values)
  R.l <- 1 - deviance/null_deviance
  R.cs <- 1 - exp(-(null_deviance - deviance) / model_N)
  R.n <- R.cs / (1 - ( exp (- (null_deviance/ model_N))))
  cat("Pseudo R^2 for logistic regression\n")
  cat("Hosmer and Lemeshow R^2 ", round(R.l, 3), "\n" )
  cat("Cox and Snell R^2 ", round(R.cs, 3), "\n" )
  cat("Nagelkerke's R^2 ", round(R.n, 3), "\n" )
}

logistic_pseudoRs(eel_model.1)
```

<!-- ### Akaike Informatin Criterion (AIC) -->
<!-- The AIC for our model is measure of fit that takes into consideration the number of predictors. The higher the number of the predictors, the more the fit is penalized. The AIC is 148.16 and can be used to compare other models in which additional predictors are included. -->

### Coefficients
The estimate or b-value for the model is 1.23. In linear regression, the coefficients represent the change in the outcome variable that can be expected after a one unit increase in a predictor variable. In logistic regression, the coefficients represent the change in the logit of the outcome variable that can be expected for the same one unit change in the predictor variable. The logit of the outcome variable corresponds to the natural logarithm of the odds of the outcome variable occurring. The z-values are normally distributed and can be tested for significance. In our example, Intervention is statistically significant which indicates that the Intervention, whether or not someone received the intervention, is making a significant contribution to the prediction of the outcome variable, Cured.

### Odds ratio
The odds ratio is the probability of something happening divided by the probability of it not happening ( p / (1-p)). In cases where the predictor variable is dichotomous, the odds ratio is defined as the exponentiated coefficient and has a straightforward interpretation. A value greater than one indicates that as the predictor increases, the odds (not to be confused with probability) of the outcome occurring increases. A value less than one indicates that as the predictor increases, the odds of the outcome occurring decreases. The odds ratio in this example is 3.42. This means that receiving an intervention increases the odds of a "Cured" outcome.
```{r, message=FALSE}
# Odds ratio
kable(exp(eel_model.1$coefficients))
```


### Confidence intervals of the odds ratio
To obtain confidence intervals for the odds ratio of model.1, simply pass the model to the `confint()` function within the `exp()` function. Our lower and upper bounds of the confidence interval does not contain 1. In fact, both of these values are greater than 1, which indicate that it is likely that receiving the experimental treatment results in increased odds of getting cured.
```{r, message = FALSE}
kable(exp(confint(eel_model.1)))
```


###

```{r, echo = FALSE, eval=FALSE}
# has_churned is the predicted prob
# most_likely outcome is the rounded predicted_prob
# odds_ratio 

# Talk about the most likely outcome because it could be easier for your audience to grasp
# Update the data frame to create the most likely outcome
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    # Add the most likely churn outcome
    most_likely_outcome = round(has_churned)
  )


# Update the plot, needs to specify aes(Y = ...) -------------------------------
plt_churn_vs_relationship +
  # Add most likely outcome points from prediction_data, colored yellow, size 2
  geom_point(aes(y = most_likely_outcome), color = "yellow", size = 2)


# Calculate the odds ratio. The odds ratio is the probability that something
# will happen over the probability that it will not happen. --------------------
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(
      mdl_churn_vs_relationship, explanatory_data, 
      type = "response"
    ),
    # Add the odds ratio
    odds_ratio = has_churned/(1-has_churned)
  )

# See the result
prediction_data

# Display the log odds ---------------------------------------------------------
# The type = "response" is different for log_odds_ratio_2
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    odds_ratio = has_churned / (1 - has_churned),
    # Add the log odds ratio from odds_ratio
    log_odds_ratio = log(odds_ratio),
    # Add the log odds ratio using predict()
    log_odds_ratio2 = predict(mdl_churn_vs_relationship, explanatory_data)
  )

# See the result
prediction_data


# Update the plot by addint the scale_y_log10()
ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dotted") +
  # Use a logarithmic y-scale
  scale_y_log10()

# Confusion matrices to assess model fit
# Create a confusion matrix to look at sensitivity and specificity
# Get the actual responses from the dataset
actual_response <- churn$has_churned

# Get the "most likely" responses from the model
predicted_response <- round(fitted(mdl_churn_vs_relationship))

# Create a table of counts
outcomes <- table(predicted_response, actual_response)

# See the result
outcomes

# Get yardstick and autoplot functions
# Convert outcomes to a yardstick confusion matrix
confusion <- conf_mat(outcomes)

# Plot the confusion matrix
autoplot(confusion)

# Get performance metrics for the confusion matrix
summary(confusion, event_level = "second")


# look at accuracy sensitivity and specificity
# Accuracy is the proportion of correct responses
# Sensitivity is the subset of those that churned
# Specificity is the subset of those that did not churn

```

```{r, include = FALSE}
colors <- c( "#440154FF","#1565c0")
# ggplot(eelData, aes(x = Duration, y = Cured, color = Intervention)) +
ggplot(eelData, aes(x = Duration, y = Cured)) +
  geom_point(alpha = 0.7, position = position_jitter(width = 0.1, height = 0.1)) +
  scale_color_manual(values = colors) +
  theme_minimal() +
  theme(axis.line = element_line(color = "grey70"))
```
### References 

<div id="refs" class="references">

<div id="ref-DSUR">

Field, Andy, Jeremy Miles, and Zoe Field. n.d. *Discovering Statistics Using R*. Sage.

</div>

<div id="ref-R-car">

Fox, John, Sanford Weisberg, and Brad Price. 2020. *Car: Companion to Applied Regression*. <https://CRAN.R-project.org/package=car>.

</div>

<div id="ref-R-tidyverse">

Wickham, Hadley. 2021. *Tidyverse: Easily Install and Load the Tidyverse*. <https://CRAN.R-project.org/package=tidyverse>.

</div>

</div>


<!-- Notes
Log-likelihood (LL): Predicted values are probabilities between 0 and 1. Log-likelihood is used assess fit of the model which is the sum of the predicted and actual outcomes. Analogous to residual sum of squares. Large values indicate a poor fit.
Deviance (-2LL): essentially a measure of model fit, calculated as -2LL
R: Partial correlation between the outcome variable and each of the predictor variables. Dependent on the Wald Statistic which can be inaccurate in some cases and should be interpreted with caution.
Pseudo R-squareds: Provide additional ways of assessing model fit, but have to be calculated by hand. There are several each with slightly different interpretations
  - Hosmer & Lemeshow: The proportional reduction in the absolute value of the log-likelihood measure. How much does the fit improve as a result of including a predictor?
  - Cox & Snell: Different mueasure, but does not reach it's theoretical maximum
  - Nagelkerke's: Modifies the R.cs value to address a limitation.
  - All of these values provide an idea of the significance of the model
AIC: Another way of assessing model fit that takes into consideration the number of predictors in the model.
Coefficients: Tell us the change in the logit of the outcome variable that can be expected for a one-unit increase in the predictor variable. Individual contributions of predictors. The z-statistic tells us if the coefficient is significantly different than zero. But when the coefficient is large, the standard error can become inflated, which underestimates the z-statistic and increases the likelihood of making a type II error (Which is a false negative, telling a pregnant woman she is not pregnant)
Odds ratio:  Calculated as the exponential of B. This indicates the change in odds resulting from a unit change in the predictor. But if the predictor is a categorical variable, . The odds value is NOT the same as probability, bc odds is calculated as P(event)/P(no event). The odds ratio is interpreted at a cutoff of 1. Greater than 1 means that as the predictor increases the odds of the outcome occurring increases. A value less than one indicates that as the predictor increases the odds of the outcome occurring decreases. 

In the second part of this series, we will create a model with an additional variable, duration, that will quantify the number of days that a participant presented with a problem prior to treatment.

-->

